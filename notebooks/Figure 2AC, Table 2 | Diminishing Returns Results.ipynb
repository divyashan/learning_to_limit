{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing broken power law + three parameter power law\n",
    "# Fit the broken power law to this set of values\n",
    "%load_ext autoreload \n",
    "%autoreload 2\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import seaborn as sns\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "sns.set_style('white')\n",
    "sys.path.insert(0, '../../')\n",
    "from curve_models import NLLS_three_param, power_law_three_param, power_law_exp_three_param\n",
    "from curve_models import NLLS_w, power_law\n",
    "from curve_models import BrokenCurve, power_law\n",
    "\n",
    "SMALL_SIZE = 14\n",
    "MEDIUM_SIZE = 16\n",
    "BIGGER_SIZE = 20\n",
    "\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "\n",
    "expmt_dict = {'gl-tiny': '0.1_5_False_uniform_30_0.5_0.5_1880_9400_94001' + '/',\n",
    "                'ml-20m-uniform': '0.1_5_False_uniform_30_0.5_0.5_20000_100000_1000001' + '/', \n",
    "                'ml-20m-tiny':'0.1_5_False_uniform_30_0.5_0.5_5250_21000_213973' + '/',\n",
    "                'gl': '0.1_5_False_uniform_30_0.5_0.5_5800_29000_290001' +'/'}\n",
    "dataset_names = ['gl', 'gl-tiny', 'ml-20m-uniform', 'ml-20m-tiny']\n",
    "dataset_dict =  {'gl':'GoogleLocal-L', 'gl-tiny':'GoogleLocal-S', \n",
    "                 'ml-20m-uniform': 'MovieLens-L', 'ml-20m-tiny': 'MovieLens-S'}\n",
    "\n",
    "# Default true slope approximation: smoothed over 4 samples\n",
    "# Alternate true slope approximation: smoothed over 2 samples\n",
    "alternate_slope_approx = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "# Plot true slope\n",
    "n_runs = 5\n",
    "slope_dicts = []\n",
    "hue_order = ['True', 'NLS_initial', 'NLS_w', 'NLS_power_law_3P', 'NLS_power_law_exp_3P', 'broken']\n",
    "for dataset_name in tqdm(dataset_names):\n",
    "    expmt = expmt_dict[dataset_name]\n",
    "    results_dir = \"../../results/forecasting/\" + dataset_name + \"/Random/\"\n",
    "    results_path = results_dir + expmt\n",
    "\n",
    "    all_ss = np.loadtxt(results_path + 'sample_sizes')\n",
    "    all_mses = np.loadtxt(results_path + 'mses')\n",
    "    data = pd.read_csv(results_path + 'results_df')\n",
    "    n_init = data['n_init'].iloc[0]\n",
    "    n_max = all_ss[0][-1]\n",
    "    for r in range(n_runs):\n",
    "        ss = all_ss[r]\n",
    "        mses = all_mses[r]\n",
    "        for i in range(1, len(ss)-1):\n",
    "            # Skip sample sizes within initially acquired set,\n",
    "            # since no data collection decision is necessary\n",
    "            if ss[i] < n_init:\n",
    "                continue\n",
    "                \n",
    "            # Record the % of data collected \n",
    "            pct = ss[i]/n_max\n",
    "            \n",
    "            # Approximate true slope at sample i \n",
    "            slope_before = (mses[i] - mses[i-1])/(ss[i] - ss[i-1])\n",
    "            slope_after = (mses[i+1] - mses[i])/(ss[i+1] - ss[i])\n",
    "            slope = .5*(slope_before+slope_after)\n",
    "            if i-2 > 0 and i+2 < len(mses) and not alternate_slope_approx:\n",
    "                slope = (mses[i+2] - mses[i-2] )/(ss[i+2] -  ss[i-2])\n",
    "            slope_dicts.append({'size': ss[i], 'slope': slope, 'run': r, 'cm': 'True',\n",
    "                               'dataset': dataset_name, 'pct': pct})\n",
    "            \n",
    "            # Naive: approximate slope with previous two subsamples\n",
    "            slope = (mses[i] - mses[i-1])/(ss[i] - ss[i-1])\n",
    "            slope_dicts.append({'size': ss[i], 'slope': slope, 'run': r, 'cm': 'Naive',\n",
    "                               'dataset': dataset_name, 'pct': pct})\n",
    "\n",
    "            # 2P-PL-Init\n",
    "            nlls = NLLS_w(power_law)\n",
    "            stop_pt = min(np.where(ss > n_init)[0])\n",
    "            nlls.fit(ss[:stop_pt], mses[:stop_pt])\n",
    "            slope = nlls.slope(ss[i])\n",
    "            slope_dicts.append({'size': ss[i], 'slope': slope, 'run': r, 'cm': 'NLS_initial',\n",
    "                               'dataset': dataset_name, 'pct': pct})\n",
    "\n",
    "            nlls_w = NLLS_w(power_law)\n",
    "            nlls_w.fit(ss[:i+1], mses[:i+1])\n",
    "            slope = nlls_w.slope(ss[i])\n",
    "            slope_dicts.append({'size': ss[i], 'slope': slope, 'run': r, 'cm': 'NLS_w',\n",
    "                               'dataset': dataset_name, 'pct': pct})\n",
    "\n",
    "            nlls_3p = NLLS_three_param(power_law_three_param, \"power_law_3p\")\n",
    "            nlls_3p.fit(ss[:i+1], mses[:i+1]) \n",
    "            slope = nlls_3p.slope(ss[i])\n",
    "            slope_dicts.append({'size': ss[i], 'slope': slope, 'run': r, 'cm': 'NLS_power_law_3P',\n",
    "                               'dataset': dataset_name, 'pct': pct})\n",
    "\n",
    "            nlls_3p = NLLS_three_param(power_law_three_param, \"power_law_3p\")\n",
    "            nlls_3p.fit(ss[:i+1], mses[:i+1]) \n",
    "            slope = nlls_3p.slope(ss[i])\n",
    "            slope_dicts.append({'size': ss[i], 'slope': slope, 'run': r, 'cm': 'NLS_power_law_3P',\n",
    "                               'dataset': dataset_name, 'pct': pct})\n",
    "\n",
    "            nlls_exp_3p = NLLS_three_param(power_law_exp_three_param, \"power_law_exp_3p\")\n",
    "            nlls_exp_3p.fit(ss[:i+1], mses[:i+1])\n",
    "            slope = nlls_exp_3p.slope(ss[i])\n",
    "            slope_dicts.append({'size': ss[i], 'slope': slope, 'run': r, 'cm': 'NLS_power_law_exp_3P', \n",
    "                               'dataset': dataset_name, 'pct': pct})\n",
    "\n",
    "            broken = BrokenCurve(power_law, \"3p-power-law\")\n",
    "            broken.fit(ss[:i+1], mses[:i+1])\n",
    "            slope = broken.slope(ss[i])\n",
    "            slope_dicts.append({'size': ss[i], 'slope': slope, 'run': r, 'cm': 'broken',\n",
    "                                'dataset': dataset_name, 'pct': pct})\n",
    "        #slopes.append(slope_before)\n",
    "slope_df = pd.DataFrame(slope_dicts)\n",
    "slope_df.to_csv('../../results/slopes_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hue_order = ['True', 'NLS_initial', 'NLS_w', 'NLS_power_law_3P', \n",
    "             'NLS_power_law_exp_3P', 'broken', 'Naive']\n",
    "fig, axs = plt.subplots(1, 4, figsize=(20, 6))\n",
    "\n",
    "colors = sns.color_palette(\"colorblind\")\n",
    "sns.set_palette([colors[3], colors[1], colors[0], colors[8], colors[9], colors[2]])\n",
    "dataset_titles = {'gl': 'GoogleLocal-L', 'gl-tiny': 'GoogleLocal-S', 'ml-20m-tiny': 'MovieLens-S',\n",
    "                 'ml-20m-uniform': 'MovieLens-L'}\n",
    "all_data_dfs  = []\n",
    "for i,dataset_name in enumerate(dataset_names):\n",
    "    ax = axs[i]\n",
    "    data_df = slope_df[slope_df['dataset'] == dataset_name]\n",
    "    sns.lineplot(x='pct', y='slope', hue='cm', hue_order=hue_order, data=data_df, \n",
    "                ax = ax)\n",
    "    ax.set_xlabel(\"% of Available Data\")\n",
    "    ax.set_title(dataset_titles[dataset_name])\n",
    "    if i == 0:\n",
    "        ax.set_ylabel(\"Slope (Return in MSE \\n for each Additional Feature-Value)\")\n",
    "    else:\n",
    "        ax.set_ylabel(\"\")\n",
    "    if i != 3:\n",
    "        ax.get_legend().remove()\n",
    "    else:\n",
    "        L=ax.get_legend()\n",
    "        new_names = ['Curve Model', 'True', '2P-Power-Law-Initial', '2P-Power-Law', '3P-Power-Law', \n",
    "                     '3P-Power-Law-Exp-Cutoff', 'Ours']\n",
    "        for i,new in enumerate(new_names):\n",
    "            L.get_texts()[i].set_text(new)\n",
    "    all_data_dfs.append(data_df)\n",
    "plt.tight_layout()\n",
    "\n",
    "all_data_df = pd.concat(all_data_dfs)\n",
    "\n",
    "suptitle = plt.suptitle(\"Minimizing by Diminishing Returns: Prediction of Returns on Data\", y=1.02)\n",
    "plt.savefig(\"../../figs/diminishing_returns_fig\", bbox_inches='tight',bbox_extra_artists=[suptitle])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['t', 'NLS_initial',  'NLS_w', 'NLS_power_law_3P', 'NLS_power_law_exp_3P', 'broken', 'Naive', 'True']\n",
    "methods = ['NLS_initial',  'NLS_w', 'NLS_power_law_3P', 'NLS_power_law_exp_3P', 'broken', 'Naive', 'True']\n",
    "final_tables = []\n",
    "for dataset in dataset_names:\n",
    "    # pcts collected for different thresholds\n",
    "    #thresholds = [-.5e-6, -.4e-6, -.3e-6, -.25e-6, -.2e-6, -.15e-6,-.1e-6,-.05e-6]\n",
    "    thresholds = [-.5e-6,   -.2e-6, -.05e-6]\n",
    "    threshold_tables = []\n",
    "    for t in thresholds: \n",
    "        dataset_df = all_data_df[all_data_df['dataset'] == dataset]\n",
    "        thresh_df = dataset_df[dataset_df['slope'] > t]\n",
    "        t_df = thresh_df.groupby(['run', 'cm']).first().reset_index()\n",
    "        t_df['t'] = t\n",
    "        t_df['dataset'] = dataset\n",
    "        threshold_tables.append(t_df)\n",
    "    threshold_table_df = pd.concat(threshold_tables)\n",
    "    # create table of means + stds over runs\n",
    "    table_df = threshold_table_df.pivot(index=['t', 'run'], columns='cm', values='pct').reset_index()\n",
    "    table_df = table_df.fillna(1)\n",
    "    table_df_means = table_df.groupby(['t']).mean().reset_index()\n",
    "    table_df_stds = table_df.groupby(['t']).std().reset_index()\n",
    "\n",
    "    for col in methods:\n",
    "        table_df_means.rename(columns={col: col+'|mean'}, inplace=True)\n",
    "        table_df_stds.rename(columns={col: col+'|std'}, inplace=True)\n",
    "    table_df_means = table_df_means.reset_index()\n",
    "    table_df_stds = table_df_stds.reset_index()\n",
    "    table_df_means_stds = table_df_means.merge(table_df_stds, on='t')\n",
    "    # turn into latex-ready table with strings\n",
    "    for method in methods:\n",
    "        mean_key = method + '|mean'\n",
    "        std_key = method + '|std'    \n",
    "        mean_values = table_df_means_stds[mean_key].round(4).map('{:.2f}'.format)\n",
    "        std_values = table_df_means_stds[std_key].round(4).map('{:.2f}'.format)\n",
    "        table_df_means_stds[method] = '$' + mean_values +  ' ± ' + std_values + \"$\"\n",
    "    final_table = table_df_means_stds[cols]\n",
    "    # Clean up column names\n",
    "    final_table_col_dict = {'t': 'Threshold', 'NLS_initial': '2P-PL-Initial',  'NLS_w': '2P-PL', 'NLS_power_law_3P': '3P-PL', \n",
    "                   'NLS_power_law_exp_3P': '3P-PL-Exp', 'broken': 'Ours', 'True': 'True',\n",
    "                           'Naive': 'Naive'}\n",
    "    final_table.rename(columns=final_table_col_dict, inplace=True)\n",
    "    final_table['Dataset'] = dataset_dict[dataset]\n",
    "    final_tables.append(final_table)\n",
    "final_tables_df = pd.concat(final_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite_e_notation(input_str):\n",
    "    if '.' not in input_str:\n",
    "        input_str =  input_str[:2] + '.0' + input_str[2:]\n",
    "    return input_str\n",
    "final_tables_df['Threshold'] = final_tables_df['Threshold'].astype(str).apply(rewrite_e_notation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_col_order = ['Dataset', 'Threshold', '2P-PL-Initial',   '2P-PL', \n",
    "                   '3P-PL', '3P-PL-Exp',  'Naive', 'Ours', 'True']\n",
    "final_tables_df = final_tables_df[final_col_order]\n",
    "latex_string = final_tables_df.to_latex(escape=False,  index=False,\n",
    "                                        column_format='l|c|ccccccc')\n",
    "latex_string = latex_string.replace('±','\\pm')\n",
    "print(latex_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Significance Analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_table_dfs  = []\n",
    "for dataset in dataset_names:\n",
    "    # pcts collected for different thresholds\n",
    "    thresholds = [-.5e-6, -.4e-6, -.3e-6, -.25e-6, -.2e-6, -.15e-6,-.1e-6,-.05e-6]\n",
    "    threshold_tables = []\n",
    "    for t in thresholds: \n",
    "        dataset_df = all_data_df[all_data_df['dataset'] == dataset]\n",
    "        thresh_df = dataset_df[dataset_df['slope'] > t]\n",
    "        t_df = thresh_df.groupby(['run', 'cm']).first().reset_index()\n",
    "        t_df['t'] = t\n",
    "        t_df['dataset'] = dataset\n",
    "        threshold_tables.append(t_df)\n",
    "    threshold_table_df = pd.concat(threshold_tables)\n",
    "    threshold_table_dfs.append(threshold_table_df)\n",
    "threshold_table_dfs = pd.concat(threshold_table_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in dataset_names:\n",
    "    print(dataset)\n",
    "    thresholds = [-.5e-6,   -.2e-6, -.05e-6]\n",
    "    dataset_df = threshold_table_dfs[threshold_table_dfs['dataset'] == dataset]\n",
    "    dataset_threshold_df = dataset_df.pivot(index=['t', 'run'], columns='cm', values='pct').reset_index()\n",
    "    dataset_threshold_df.fillna(1, inplace=True)\n",
    "    baselines = ['NLS_initial', 'NLS_power_law_3P', 'NLS_w', 'NLS_power_law_exp_3P', 'Naive']\n",
    "    for t in thresholds:\n",
    "        threshold_results = dataset_threshold_df[dataset_threshold_df['t'] == t]\n",
    "        for baseline in baselines:\n",
    "            our_error = np.abs(threshold_results['broken'] - threshold_results['True'])\n",
    "            baseline_error = np.abs(threshold_results[baseline] - threshold_results['True'])\n",
    "            c, p_val = ttest_ind(our_error, baseline_error)\n",
    "            if p_val > .05:\n",
    "                print(baseline, t)\n",
    "                print(list(threshold_results['broken']))\n",
    "                print(list(threshold_results[baseline]))\n",
    "                print(list(threshold_results['True']))\n",
    "\n",
    "                print(np.mean(list(threshold_results['broken'])), np.mean(threshold_results['True']))\n",
    "                print(np.mean(list(threshold_results[baseline])), np.mean(threshold_results['True']))\n",
    "        print('--')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
